\documentclass[a4paper,12pt,oneside]{book}
\usepackage[usenames]{color} %usato per il colore
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage[latin1]{inputenc}
\usepackage[pdftex]{graphicx} 
\usepackage{filecontents}
\usepackage[italian]{babel}
\usepackage{setspace}

\graphicspath{ {./../Immagini/} }

\begin{document}

\addtolength{\oddsidemargin}{+1,0cm} 
\addtolength{\evensidemargin}{+1,0cm} 
\onehalfspacing

%\tableofcontents
%\listoftables
%\listoffigures
%\newpage

Siamo sommersi dai dati.
\\\\
Ogni giorno viene generata una quantità enorme di dati da azioni della vita quotidiana: dalle applicazioni per smartphone alle carte di credito usate per gli acquisti, dai programmi eseguiti sui computer ai sensori utilizzati nelle infrastrutture intelligenti della città.
\\
Le grandi quantità di dati sopra citate vengono chiamate Big Data.
\\
In generale, con il termine Big Data si intende una collezione di dati talmente estesa in termini di volume, velocità e varietà da richiedere metodologie e tecnologie non convenzionali (i.e. nuove) per la loro memorizzazione, gestione, interrogazione ed analisi. Queste grandi raccolte di dati vengono generati sia manualmente che automaticamente: i due rispettivi casi tipici sono i click che gli utenti effettuano durante la navigazione tra le pagine web e gli smartphone che dialogano con specifiche applicazioni, inviando, per esempio, dati riguardanti la nostra posizione.
\\
I Big Data sono caratterizzati da tre aspetti importanti, chiamati anche \textbf{3V}:
\begin{itemize}
	\item \textit{Volume}. Rappresenta la quantità dei Big Data. Ogni giorno vengono prodotti dati nell'ordine dei Terabytes e dei Petabytes, che devono essere salvati oppure processati e consumati in tempo reale. Entrambi sono casi problematici: se i Big Data devono essere salvati in qualche base di dati il problema è nel salvataggio; se, invece, devono essere consumati immediatamente, il problema risiede nella loro analisi massiva.
	\item \textit{Velocità}. Rappresenta il tempo per la generazione dei dati. Significa sia quanto velocemente questi dati sono stati prodotti, sia quanto velocemente i dati devono essere processati per soddisfare un qualche obiettivo o domanda. Per gestire questa caratteristica bisogna capire se i dati catturati devono essere salvati in una base di dati o direttamente processati: nel primo caso bisogna utilizzare un database che permetta di effettuare le operazioni di fetching e di inserimento ad alte velocità; nel secondo caso, invece, bisogna utilizzare un'infrastruttura che sia capace di gestire le massive operazioni che devono essere effettuate sui Big Data.
	\item \textit{Varietà}. Rappresenta la tipologia dei dati, che provengono da fonti diverse: strutturate e non strutturate. Con dati strutturati si intendono dati che sono organizzati secondo schemi rigidi, che vengono generalmente salvati in basi di dati; con dati non strutturati, invece, sono dati non schematizzati, che vengono generalmente salvati in file. La varietà dei Big Data è data dalla loro non strutturazione: blog post e commenti sui social network ne sono qualche esempio.
\end{itemize}
L'obiettivo di aziende ed organizzazioni è quello di estrarre nuova conoscenza da questa enorme mole di dati, che viene usata per estendere quella preesistente e per facilitare i processi decisionali. A causa delle caratteristiche dei Big Data, il processo di estrazione è impegnativo, non solo per la grandezza e per il tempo di generazione dei dati, ma soprattutto perché la maggior parte dei Big Data sono di tipo non strutturati.
\\\\
Questo problema è particolarmente sentito nel World Wide Web. Il Web è il più grande, eterogeneo e dinamico contenitore di risorse liberamente fruibile da chiunque, composto da pagine Web ricche di contenuti informativi memorizzati in vario formato: testi, immagini, audio, video e link, che consentono di esplorare pagine già preesistenti nel web o pagine dello stesso sito. 
\\
Anche nel Web, i dati da cui estrarre informazioni utili, per la maggior parte di essi, sono di tipo non strutturato. Per questo, il processo di estrazione di nuova conoscenza dai siti Web è una sfida impegnativa: data l'eterogeneità dei dati che caratterizzano il sito Web e la loro struttura hyperlinks si sono dovute sviluppare nuove metodologie di estrazione del sapere.
\\\\
In questo campo sono state effettuate ricerche e sperimentazioni che hanno evoluto il Data Mining.

\section{Data Mining}
Il Data Mining è l'insieme di tecniche che hanno come obiettivo l'estrazione del sapere o della conoscenza, partendo da grandi quantità di dati.
\\
Il termine significa letteralmente "estrazione di dati" e si divide in:
\begin{itemize}
	\item \textbf{estrazione}: l'informazione implicita, nascosta o formata da dati strutturati viene estratta per renderla immediatamente utilizzabile;
	\item \textbf{esplorazione ed analisi}: vengono scoperti pattern significativi, per mezzo dei quali si estrae l'informazione significativa. Con il termine pattern, si intende uno schema, una regolarità, o, in generale, una rappresentazione sintetica dei dati \cite{cinecadatamining}.
\end{itemize}
L'informazione viene considerata significativa in base al dominio applicativo in cui si utilizzano le metodologie di Data Mining. L'estrazione di nuovo sapere varia molto a seconda del campo applicativo. Le differenze sono tali da dover suddividere tali procedimenti in diverse aree, in base al tipo di dato da cui estrarre informazioni utili.
\\
Dai testi è possibile estrarre conoscenza utile. Il ramo del Data Mining che si occupa di realizzare questo processo è il Text Mining.
\\\\
Per definizione, il Text Mining, chiamato anche Text Data Mining o Text Analysis, è l'applicazione delle tecniche e metodologie del Data Mining ai testi. L'obiettivo è lo stesso del Data Mining: estrarre informazioni latenti utili per ampliare conoscenze pregresse e facilitare i processi di decisione partendo da documenti e testi, trasformandoli in sapere che può essere usato per successive analisi. Il Text Mining effettua tali indagini attraverso l'applicazione di metodologie: il Natural Language Processing ne è un esempio.
\\\\
Il Natural Language Processing è il processo di trattamento automatico mediante un calcolatore delle informazioni scritte o parlate in una lingua naturale, aiutando una macchina a "leggere" dei testi. 
\\
La sua difficoltà è l'elevato numero di ambiguità che caratterizza il linguaggio umano. Per questo motivo è stato diviso in quattro fasi, o sottoprocessi:
\begin{itemize}
	\item \textbf{analisi lessicale}: in questa fase avviene la scomposizione della sequenza di caratteri, chiamata espressione linguistica, in token (o parole);
	\item \textbf{analisi grammaticale}: in questa fase avviene l'associazione di ciascuna parola ad una parte del discorso;
	\item \textbf{analisi sintattica}: in questa fase avviene il parsing dei token e viene generato un albero di parser (parse tree);
	\item \textbf{analisi semantica}: in questa fase viene assegnato un significato al parse tree, che provvede alla disambiguazione dell'espressione linguistica, ovvero ad assegnare un significato tra quelli disponibili.
\end{itemize}
Il testo non è la sola fonte da cui estrarre conoscenza: anche sulle pagine Web è possibile applicare, con dovute modifiche e rivisitazioni, le tecniche di Data Mining per rilevare informazioni utili, anche applicando il Natural Language Processing \cite{ijcatr04031008}.

\section{Web Mining}
Con l'espressione Web Mining ci si riferisce all'applicazione di procedure analoghe a quelle del Data Mining per estrarre automaticamente informazioni dalle risorse presenti nel Web, siano essi documenti o servizi \cite{webminingmalerba}. 
\\
Secondo \cite{webminingmalerba}, per estrarre informazioni utili da pagine Web, è necessario seguire specifici passi:
\begin{itemize}
	\item \textbf{Scoprire le risorse}: gli strumenti per la scoperta di risorse, che vengono chiamati Spider, ovvero Web Robot, scandiscono milioni di documenti Web e costruiscono indici di ricerca in base alle parole che si trovano negli stessi.
	\item \textbf{Estrarre le informazioni}: i testi, che sono scritti in linguaggio naturale, vengono trasformati in rappresentazioni strutturate predefinite, dette template, che rappresentano un estratto dell'informazione presente nel testo.
	\item \textbf{Generalizzare}: i processi di navigazione nel Web devono essere generalizzati, ovvero applicabili in altri contesti.
\end{itemize}

In altri termini, il Web Mining è l'applicazione delle procedure di Data Mining per scoprire pattern dal World Wide Web ed estrarre informazioni utili. 
\\\\
Il Web è caratterizzato da proprietà che lo differenziano da altre sorgenti dati, e sono:
\begin{itemize}
	\item \textbf{Dimensione}. Il Web è il primo mezzo in cui il numero di produttori di informazioni è uguale al numero di consumatori. La quantità di dati e di informazioni sul Web è enorme ed è in continua crescita.
	\item \textbf{Dinamicità}. Ogni secondo vengono create, distrutte e modificate migliaia di pagine. Queste azioni rendono il Web una rete informativa dinamica, in cui il contenuto e la struttura cambiano con frequenza. Tenere traccia, quindi, di questi cambiamenti e monitorarli rimane una sfida impegnativa per molte applicazioni.
	\item \textbf{Eterogeneità}. Il Web è eterogeneo, e tale caratteristica dipende fortemente sia dal formato delle pagine sia dal contenuto testuale.
	\\
	Nel primo caso, l'eterogeneità è definita dal fatto che non esiste uno standard di formato, dividendo le pagine web in 3 tipologie: \textit{pagine non strutturate}, \textit{pagine strutturate} e \textit{pagine semi-strutturate}.
	\\
	Le pagine \textit{non strutturate} sono scritte in linguaggio naturale, non sono caratterizzate da nessuna struttura e possono essere applicate tecniche di estrazione dell'informazione con un certo grado di affidabilità.
	\\
	Le pagine \textit{strutturate} vengono generate normalmente da una sorgente dati di tipo strutturato (e.g. database): i dati vengono pubblicati una volta che vengono inseriti in una qualche struttura (e.g. forma tabellare). In questo caso, l'estrazione della conoscenza viene effettuata attraverso l'individuazione di regole sintattiche.
	\\
	Le pagine \textit{semi-strutturate} sono una via di mezzo delle tipologie descritte in precedenza: sono caratterizzate dalla presenza sia di sezioni strutturate che da testo libero. L'estrazione della conoscenza viene effettuata cercando dei pattern nei tag HTML, utilizzando i metadati o identificando solo l'informazione strutturata.
	\\
	Nel secondo caso, l'eterogeneità del Web è definita dal fatto che le pagine vengono create da milioni di persone aventi differente cultura, abilità e linguaggio. Da questo si deduce che le pagine Web possono avere informazioni simili o uguali, ma presentata in maniera completamente differente.
	\item \textbf{Connessione}. Il Web viene generalmente rappresentato come una rete di informazioni, in cui i nodi sono le pagine Web e gli archi gli hyperlink o collegamenti ipertestuali. Questi collegamenti hanno caratteristiche e funzionalità differenti in base al loro utilizzo, che può essere sia per connettere pagine di uno stesso sito, sia pagine di siti differenti. All'interno del sito, i link servono per organizzare i contenuti; fra siti diversi, invece, vengono usati per collegare argomenti simili o inerenti a quelli della pagina di partenza.
	\item \textbf{Rumore}. Il Web, a differenza da altri mezzi di informazione, ha la caratteristica di permettere a chiunque di pubblicare contenuti senza alcun tipo di approvazione. Questo permette al Web di espandersi enormemente e di arricchire e diversificare le informazioni, ma contribuisce anche alla creazione e diffusione di contenuti di bassa qualità, rindondanti ed erronei.
	\item \textbf{Società virtuale}. Il Web può essere considerato come un grande Social Network, dove le persone possono diffondere la loro conoscenza ed influenzarsi reciprocamente. Infatti non riguarda solo i dati, le informazioni o i servizi, ma anche le interazioni fra persone, organizzazioni o sistemi automatizzati.
\end{itemize}

Sulla base di queste caratteristiche, per raggiungere l'obiettivo del Web Mining si possono usare metodologie diverse che permettono di individuare informazioni utili di tipo differente. Queste vengono estratte partendo dalla struttura degli hyperlink, dal contenuto e dall'uso della pagina Web. Quindi, il Web Mining può essere suddiviso in tre distinte categorie: Web Structure Mining, Web Content Mining e Web Usage Mining.

\begin{figure}[h]
	\centering
	\includegraphics[width = 95mm]{webminingtax}
	\caption{Categorie di Web Mining}
	\label{webminingtax}
\end{figure}

\paragraph{Web Structure Mining}
Il Web Structure Mining è un processo di estrazione di informazioni utili partendo dalla struttura ad hyperlink di un sito Web, che viene considerato come un grafo, i cui nodi sono le pagine e gli archi sono gli hyperlink tra le pagine.
\\
Basata sulla topografia degli hyperlink, il Web Structure Mining può categorizzare le pagine web e generare informazioni come la similarità e le relazioni tra i differenti siti Web \cite{Nilima}.
\\
Tecniche tradizionali di Data Mining non possono generare conoscenza utile perché non è presente una struttura a link in una tabella relazionale (i.e. database).
\\
Tra i più importanti algoritmi che appartengono a questa tipologia si possono trovare Page Rank \cite{pagerankstanford} e HITS \cite{Kleinberg99}, i quali sfruttano la struttura ad hyperlink del Web per assegnare un rank alle pagine, ovvero per restituirle in ordine di importanza relativamente ad una determinata query.

\paragraph{Web Content Mining}
Il Web Content Mining viene usato per cercare informazioni utili dai contenuti delle pagine Web, che possono essere collezioni di testi, immagini, audio, video o dati strutturati che sono incapsulati in liste e tabelle.
\\
Per estrarre il sapere da contenuti più complessi, come le immagini, le tecniche di Web Content Mining sono molto limitate \cite{webmining21cap}.
\\
Le tecniche di questa tipologia di Web Mining possono sembrare abbastanza simili alle metodologie tradizionali di Data Mining o di Text Mining, ma le caratteristiche delle pagine Web (e.g. presenza di tag HTML) non permette a queste di essere direttamente applicabili sulle stesse.

\paragraph{Web Usage Mining}
Il Web Usage Mining è l'applicazione delle tecniche di Data Mining per la scoperta di pattern e informazioni utili attraverso l'analisi dei log, che sono immagazzinati nei Web server o nei  sistemi che tracciano le attività degli utenti.
\\
L'obiettivo di questo campo è la profilazione dell'utente, ovvero analizzare i suoi comportamenti sul web, sia per comprendere quali sono i suoi reali bisogni, sia per offrire dei servizi che possano soddisfare tali necessità e personalizzare l'esperienza Web.
\\
Questo tipo di Data Mining viene usato in campi disparati, che vanno dalle aziende alle agenzie governative: ad esempio, i siti di e-commerce usano questo tipo di tecnologia per presentare all'utente prodotti per i quali potrebbe essere interessato; le agenzia governative, invece, usano il Web Mining anche per classificare minacce e attentati terroristici.
\\
Alcuni, però, criticano questa tecnologia: il problema etico di cui più si parla è la violazione della privacy, con il rischio di vedere diffusi i propri dati, anche sensibili, senza alcuna consapevolezza da parte dell'utente \cite{612259}.

\section{Rappresentazioni vettoriali di pagine Web}
L'estrazione di informazione utile, a partire da pagine Web di tipo non strutturato o semi-strutturato è un processo impegnativo, a causa del ridotto numero di pattern che possono essere usati per ricavare conoscenza.
\\
Un modo per affrontare questa sfida è l'applicazione di algoritmi di Machine Learning che possano individuare schemi per estrapolare informazioni utili a partire dai contenuti e dalla struttura delle pagine Web.
\\
Per definizione, il Machine Learning è un campo di ricerca, appartenente all'Informatica, che si occupa dello studio della scoperta di pattern. I suoi task sono tipicamente classificati in due categorie, che dipendono dalla natura dei dati che devono essere appresi dal sistema:
\begin{itemize}
	\item \textbf{Supervised learning}. E' chiamato anche apprendimento supervisionato, in cui un computer deve dedurre una funzione partendo da dati di apprendimento etichettati. Tali dati sono una collezione di esempi. In questo tipo di apprendimento, ogni esempio viene trattato come una coppia di input/output, in cui il primo è l'etichetta del dato che si vuole predire e il secondo è il valore che l'algoritmo predice.
	\\
	Il computer analizza i dati di apprendimento e produce una funzione dedotta, che può essere usata su nuovi dati.
	\item \textbf{Unsupervised learning}. E' chiamato anche apprendimento non supervisionato, il cui il computer deve dedurre una funzione che descriva la struttura nascosta di dati non etichettati. 
	\\
	Dato un campione in input, non vi sono errori o segnali per valutare una risposta da parte del sistema. In questa categoria rientrano le reti neurali (si vedrà un esempio nella Sezione \ref{word2vec}) e gli algoritmi di clustering (in Sezione \ref{clustering}).
\end{itemize}
La maggior parte degli algoritmi di Machine Learning trasformano il contenuto di un testo in uno spazio vettoriale \cite{live-2934-4846-jair}, permettendo di estrapolare l'informazione nascosta da testi scritti in linguaggio umano.
\\
In questa tesi viene proposto l'utilizzo del modello di spazi vettoriali per rappresentare i vertici del grafo della pagina Web. Per spiegare che cos'è e perché è importante il modello di spazio vettoriale, occorre specificare e descrivere l'argomento di quello dello spazio delle parole (Word space model), che sono strettamente collegati l'uno all'altro.

\subsection{Word space model}
Il Word Space Model, come definito in \cite{wordspacemodel}, è una rappresentazione spaziale del significato delle parole. Si basa sul fatto che la similarità semantica viene rappresentata come prossimità, in uno spazio ad $n$ dimensioni, dove $n$ è un intero.

\begin{figure}[h]
	\centering
	\includegraphics[width = 50mm]{wordspaceexample}
	\caption{Esempi di spazi di parole, rispettivamente mono-dimensionale (1) e bi-dimensionale (2)}
	\label{wordspaceexamples}
\end{figure}

In Figura \ref{wordspaceexamples} sono riportati due esempi di spazi di parole, in cui la prossimità è data dalla posizione delle parole nello spazio.
In entrambi i casi, si può notare come il termine $sitar$ sia più simile di significato a $oud$, e meno simile a $guitar$.
\\\\
Ma come si costruisce uno spazio delle parole? Una modalità di costruzione è la \textbf{matrice di co-occorrenze}.
\\
Tale matrice può essere formata sia parola per parola ($w$ x $w$), dove $w$ sono i tipi di parole nel set, sia parola per documento ($w$ x $d$), dove $d$ sono i documenti nel set. Le celle di questa matrice registrano la frequenza di occorrenza della parola i-esima in un contesto j-esimo, oppure in un documento j-esimo nel caso di matrice parola per documento.
\\
Con il termine \textit{contesto} si intende un insieme di parole che si trovano nelle vicinanze della parola i-esima.
\\\\
Per spiegare la matrice di co-occorrenze prendiamo come esempio la frase:
\begin{figure}[h]
	\centering
	Whereof one cannot speak thereof one must be silent
	\label{sentencematrix}
	\caption{Frase di esempio per la matrice di co-occorrenze \cite{wordspacemodel}}
\end{figure}

\begin{table}[h]
	\centering
	\caption{Matrice di co-occorrenze parola per parola \cite{wordspacemodel}}
	\label{tablematrix}
	\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
		\hline
		& whereof & one & cannot & speak & thereof & must & be & silent  \\ \hline
		whereof & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0  \\ \hline
		one & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 \\ \hline
		cannot & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\ \hline
		speak & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\ \hline
		thereof & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 \\ \hline
		must & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 0 \\ \hline
		be & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\ \hline
		silent & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\ \hline
	\end{tabular}
\end{table}

Le celle della matrice di co-occorrenze, visibile nella Tabella \ref{tablematrix}, registrano le occorrenze delle parola i-esima, le quali dipendono dal contesto.\\
Queste liste di occorrenze sono dei veri e propri vettori. Un vettore, per definizione, è un elemento di uno spazio vettoriale, ed è definito da $n$ componenti o coordinate $\overrightarrow{v} = (x_1, x_2, ..., x_n)$ \cite{wordspacemodel}. Tali coordinate definiscono la posizione nello spazio n-dimensionale.
\\
Quindi, la matrice di co-occorrenze non è altro che una realizzazione del modello dello spazio vettoriale, chiamato Vector Space Model.

\paragraph{Vector Space Model}
\label{tfidf}
Il Vector Space Model è una modalità algebrica per rappresentare documenti di testo come vettori, inseriti in uno spazio vettoriale.
\\
Generalmente, per definire i vettori viene usato come peso \textit{tf-idf}, una funzione che viene usata per misurare l'importanza di un termine rispetto ad un documento o ad una collezione di documenti. Come suggerisce il nome, è composta da due fattori: \textit{tf} e \textit{idf}.
\\
\textit{Tf}, abbreviazione di \textit{term frequency}, misura quante volte un termine appare in un documento. Dato che ogni documento ha lunghezza differente, ovvero è composto da un differente numero di parole, è possibile che un termine possa apparire molte più volte nei documenti più lunghi rispetto a quelli più corti. Questo problema viene risolto dividendo la frequenza dei termini per la lunghezza del documento. La formula è:
\begin{equation}
	tf_{i, j} = \frac{n_{i, j}}{|d_J|}
\end{equation}
dove $n_{i, j}$ è il numero di occorrenze del termine $t_i$ che si trova nel documento $d_j$, mentre il denominatore è la dimensione del documento $d_j$.
\\
\textit{Idf}, abbreviazione di \textit{inverse document frequency}, misura invece i termini che si presentano più volte in un documento, ma con meno frequenza in tutta la collezione di documenti. Questo perché potrebbero esserci dei termini  più significativi che appaiono raramente in un determinato elaborato, ma frequentemente nel set di documenti. La formula è:
\begin{equation}
	idf_i = \log{\frac{|D|}{|\{d : t_i \in d\}|}}
\end{equation}
dove $|D|$ è il numero di documenti presenti nella collezione, mentre il denominatore è il numero di documenti che contengono il termine $t_i$.
\\\\
Il modello di spazio vettoriale sopra descritto, però, presenta particolari problematiche quando si vuole costruire uno spazio delle parole: da un lato, se non si hanno sufficienti dati, non sarà possibile costruire, in maniera fedele, un modello di distribuzione delle parole; dall'altro, se tale spazio vettoriale sarà dimensionalmente grande, innalzerà la complessità computazionale.
\\
Secondo la sperimentazione portata avanti da \cite{wordspacemodel}, è stato riscontrato un altro problema: nella matrice di co-occorrenze circa il 99\% delle celle conterrà zero come valore. Solo una parte delle parole apparirà realmente. Questo fenomeno è un esempio dell'applicazione della \textit{Zipf's law}, una legge empirica che deriva dalla frequenza di un evento $P_i$, che fa parte di un insieme, in funzione della posizione $i$ chiamata \textit{rango} nell'ordimanto decrescente rispetto alla frequenza stessa di tale evento. La formula della legge è la seguente:
\begin{equation}
	f(P_i) = \frac{c}{i}
\end{equation}
dove $i$ indica il rango, $P_i$ indica l'evento che occupa l'i-esimo rango (ovvero l'i-esimo evento più frequente), $f(P_i)$ è il numero di volte (frequenza) che si verifica l'evento $P_i$, c è una costante di normalizzazione, pari al valore $f(P_i)$.
\\\\
Una possibile soluzione alle situazioni problematiche sopra descritte è la riduzione della dimensione (\textit{dimensional reduction}): un processo di compressione di uno spazio multi dimensionale ad uno avente bassa dimensione, causando anche la riduzione della grandezza della matrice e del numero di zero ivi inseriti.
\\
Una tecnica di riduzione della dimensione più utilizzata è \textit{t-SNE}, che è particolarmente adatta per comprimere uno spazio vettoriale di grandi dimensioni in uno avente uno, due o tre dimensioni, per poi essere visualizzato in un grafico di dispersione. Questo tipo di grafico è un sistema gli assi cartesiani di uno, due o tre dimensioni. Una volta effettuata la riduzione, i valori ottenuti per ciascun elemento del set vengono usati per posizionarlo nello spazio.

\begin{figure}[h]
	\centering
	\includegraphics[width = 160mm]{tsne}
	\caption{Esempio di grafico di dispersione}
	\label{tsne}
\end{figure}

\paragraph{Word embedding}
La funzione peso \textit{tf-idf} non è la sola usata per costruire uno spazio dei vettori: nelle tecniche di Data Mining, che sfruttano gli algoritmi di Machine Learning, ci si sta orientando sempre più sull'uso delle reti neurali per estrarre conoscenza partendo da una collezione di dati.
\\
Questo è il Word Embedding, nome di una serie di tecniche per il language modeling e per il Feature Learning nel campo del Natural Language Processing \cite{bengio03}, in cui ad ogni parola viene associato un vettore chiamato \textit{Feature Vector}.
\\\\
Il Word Embedding è una funzione parametrizzata
\begin{equation}
	W : words \to \mathbb{R^n}
\end{equation}
che trasforma le parole di un dato linguaggio in un vettore multidimensionale. Per esempio:
\begin{equation}
	W("mat") = (0.0, 0.6, -0.1, ...)
\end{equation}
\\
Partendo da un documento, è possibile trasformare i termini in vettori, formando un vero e proprio spazio vettoriale, chiamato anche Word Embeddings Space.

\begin{figure}[h]
	\centering
	\includegraphics[width = 150mm]{tsneplot}
	\caption{Esempio di spazio degli embeddings}
	\label{tsneplot}
\end{figure}

In Figura \ref{tsneplot} è possibile notare come parole simili si trovano vicine tra loro: la parola \textit{three} è molto vicina alle parole \textit{two} e \textit{four}. Questo è dovuto al fatto che tali parole hanno vettori simili. Infatti, se si usa un sinonimo, la validità della frase non cambia (per esempio: "poche persone cantano bene" $\to$ "un paio di persone cantano bene"), perché le parole "poche" e "paio" sono vicine tra loro ed hanno vettori simili.
\\\\
Un'altra proprietà interessante della funzione di Word Embedding è l'analogia tra le parole, nascosta nella differenza dei loro vettori:
\begin{equation}
	W("woman") - W("man") \simeq W("queen") - W("king")
\end{equation}
Da questo si evince che c'è una correlazione tra parole che hanno genere opposto, in quanto appariranno in contesti simili, differenti solo per alcuni dettagli come pronomi o articoli. Lo stesso principio vale per parole singolari e plurali \cite{mikolov13}.
\\\\
Apprendere dei termini e trasformarli in Feature Vectors rappresenta una base per effettuare operazioni di Data Mining, come per esempio il raggruppamento dei termini appresi in gruppi attraverso il clustering, usando qualche funzione di similarità. Si approfondiranno tali funzioni nella Sezione \ref{distancefunctions}.

\subsection{Word2Vec}
\label{word2vec}
Word2Vec è un algoritmo di Word Embedding ed è una rete neurale a due livelli che apprende le parole da un testo in input, le quali vengono trasformate in vettori chiamati Feature Vectors. Viene considerato erroneamente come un deep-learning (apprendimento approfondito): in realtà si tratta di un apprendimento di tipo superficiale (shallow-learning).
\\\\
L'output di questa rete neurale è un vocabolario in cui ogni termine ha un vettore, che può essere compreso da una rete di deep-learning o semplicemente interrogato per rilevare delle relazioni tra i termini.
\\\\
Word2Vec è composto da due modelli di apprendimento:
\begin{itemize}
	\item \textbf{CBOW} (continuous bag of words)
	\item \textbf{Skip-Gram}
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width = 150mm]{word2vecmodels}
	\caption{I modelli di apprendimento di Word2Vec}
	\label{word2vecmodels}
\end{figure}

\textbf{CBOW} consiste nel predire una determinata parola a partire dal suo contesto, che è composto dal numero di parole che vengono prese in considerazione durante l'apprendimento. Questo modello di apprendimento tratta l'intero contesto come una sola osservazione. Generalmente, CBOW restituisce risultati più accurati con piccole collezioni di dati.
\\\\
\textbf{Skip-Gram}, invece, è l'inverso di CBOW: predice il contesto a partire da una parola.\\
Questo modello di apprendimento tratta ogni coppia contesto-obiettivo come una nuova osservazione, rendendo i risultati più accurati quando si hanno grandi collezioni di dati.\\
Per capire meglio questo tipo di modello di apprendimento, analizziamo questa frase:
\begin{figure}[h]
	\centering
	The quick brown fox jumped over the lazy dog.
\end{figure}
\\
Inizialmente, si crea il set di dati formato da coppie (contesto, parola), di cui il primo è una sequenza di parole che dipende dalla dimensione della finestra, mentre la parola è il termine che si sta esaminando. Quindi, se si ha una finestra di contesto di dimensione 1, il set di dati sarà:
\begin{figure}[h]
	\centering
	([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox) $\to$ (quick, the), (quick, brown), (brown, quick), (brown, fox)
\end{figure}

\subsection{LINE}
\label{line}
LINE è un nuovo modello di network embedding, capace di apprendere rappresentazioni mono-dimensionali di vertici in una rete o grafo. Un grafo è una coppia ordinata $G = (V, E)$ di insiemi, con $V$ insieme dei nodi ed $E$ insieme degli archi.
\\
Questo modello di apprendimento lavora bene soprattutto con grafi orientati, pesati e non-pesati. In \cite{tang2015line} viene effettuata una sperimentazione in cui sono state valutate le prestazioni di LINE nelle reti di informazioni del mondo reale, come social networks o ancora citation networks. Dato che LINE apprende un grafo, deve anche preservare la prossimità di primo e secondo ordine separatamente.

\begin{figure}[h]
	\centering
	\includegraphics[width = 100mm]{infonet}
	\caption{Un esempio di grafo/network di informazioni \cite{tang2015line}}
	\label{infonet}
\end{figure}

Per spiegare la prossimità di primo e secondo ordine, analizziamo la Figura \ref{infonet}. I vertici 6 e 7 sono collegati da un arco avente un determinato peso: tale peso indica la prossimità di prim'ordine. Nel caso dei vertici 6 e 8, dato che non esiste un arco tra questi due, la prossimità di primo ordine è 0.
\\
I vertici 5 e 6, invece, condividono molti vertici vicini: hanno un'alta prossimità di secondo ordine.
\\
Se due vertici hanno un'alta prossimità dovrebbero essere rappresentati nello spazio degli embeddings vicini tra loro.
Si noti come la prossimità di primo ordine può essere applicata solo a grafi non orientati; la prossimità di secondo ordine, invece, è applicabile sia a grafi orientati che non. Ancora, in \cite{tang2015line} si è osservato come la prossimità di primo ordine, nel mondo reale, non è sufficiente per preservare le strutture del network globali. Per questo motivo, nella sperimentazione effettuata in \cite{tang2015line}, è stata esplorata la prossimità di secondo ordine.
\\
Tali prossimità, tuttavia, sono complementari l'una all'altra.

\subsection{Doc2Vec}
\label{doc2vec}
Doc2Vec, chiamato anche Paragraph2Vec, è un'estensione di Word2Vec che apprende correlando etichette e parole, invece che parole con altre parole.
\\
Differentemente da Word2Vec, che converte una parola in un vettore, Doc2Vec aggrega tutte le parole di una frase in un vettore.
\\
Quindi, data una collezione di testi che possa essere divisa in $n$ documenti, o paragrafi, ad ogni paragrafo è assegnato un vettore. Il processo di apprendimento è caratterizzato dallo spostamento della finestra delle parole di contesto attraverso ogni parola di ogni paragrafo, per ogni paragrafo \cite{HongSeokho}.

\section{Clustering}
\label{clustering}
Le classi, insiemi di oggetti che condividono determinate caratteristiche, hanno un ruolo importante sia nell'analisi effettuata da persone, sia nella descrizione del mondo. Infatti, gli esseri umani hanno l'abilità di dividere gli oggetti in gruppi e di assegnarli a questi insiemi \cite{ch8}. 
\\
Questo processo di divisione prende il nome di Clustering ed ha come scopo quello di selezionare e raggruppare, da una collezione di dati, elementi omogenei, avendo come base la somiglianza tra gli stessi.
\\
L'attività di raggruppamento può essere applicata in molti campi \cite{ch8}:
\begin{itemize}
	\item \textbf{Biologia}. La ricerca scientifica ha investito molto tempo per creare una tassionomia di tutte le specie viventi, basandosi sul regno, classe, ordine, famiglia, geni e specie. Non solo, tecniche di clustering sono state applicate per analizzare una grande quantità di informazioni genetiche: sono stati raggruppati geni che hanno funzioni simili.
	\item \textbf{Clima}. Per capire il clima della Terra bisogna trovare degli schemi nell'atmosfera e negli oceani. Il Clustering è stato applicato per scoprirli, analizzando la pressione atmosferica delle regioni polari e nelle aree di oceano che hanno un impatto significativo sul clima della terra ferma.
	\item \textbf{Psicologia e Medicina}. Una malattia ha un incredibile numero di variazioni, e le operazioni di Clustering possono essere usate per identificare queste sottocategorie. Per esempio, è possibile applicare questo processo per scoprire tipologie differenti di depressione.
	\item \textbf{Business}. Il campo del Business raccoglie una quantità enorme di informazioni di clienti potenziali ed effettivi. Il Clustering può essere usato per dividere i consumatori in gruppi per successive attività di analisi e di marketing.
	\item \textbf{Information Retrieval}. Il World Wide Web è un enorme contenitore di pagine Web, ed una semplice ricerca può dare come risultato milioni di pagine. Le tecniche di Clustering vengono usare per dividerle in gruppi, ognuno dei quali cattura un aspetto particolare dell'interrogazione. Quindi, se si cerca, con un motore di ricerca, la parola \textit{film}, verranno restituite pagine Web raggruppate in categorie come recensioni, trailer, celebrità, teatri e cinema. Ogni categoria (cluster) può essere diviso in sottogruppi (sottocluster) e si produrrà una struttura gerarchica che servirà all'utente per effettuare altre ricerche.
\end{itemize}

\subsection{Approcci di clustering}
L'operazione di clustering è essenzialmente la creazione di un insieme di cluster (i.e. un insieme di insiemi), che generalmente contengono tutti gli elementi iniziali.
\\
Si possono usare varie classi di approcci per effettuare clustering su un determinato insieme di dati iniziali. Alcune di queste sono:
\begin{itemize}
    \item Hard clustering o soft clustering
    \item Partizionali o gerarchici
\end{itemize}

\paragraph{Hard clustering e soft clustering}
Questi algoritmi attuano un approccio secondo cui un elemento può essere assegnato ad un solo cluster o a più cluster. Con hard clustering intendiamo che l'algoritmo assegna un elemento ad uno ed un solo cluster; con soft clustering, invece, l'elemento può essere assegnato a più cluster con gradi di appartenenza diversi.

\paragraph{Clustering partizionale}
Gli algoritmi di clustering partizionali creano una divisione delle osservazioni minimizzando una certa funzione di costo:
\begin{equation}
   {\textstyle \sum_{j=1}^{k}} E(C_j)
\end{equation}
dove $k$ è il numero desiderato di cluster, $C_j$ è il j-esimo cluster ed $E : C \to \mathbb{R^+}$ è la funzione di costo associata al singolo cluster. L'algoritmo più famoso che fa parte di questa categoria è K-Means.

\paragraph{Clustering gerarchico}
Gli algoritmi facente parti di questa categoria non suddividono lo spazio, bensì costruiscono una gerarchia di cluster. In questa strategia rientrano due sottotipi:
\begin{itemize}
\item \textbf{Aggregativo}: tale approccio considera n cluster per n elementi, cioè ogni elemento viene considerato un cluster a sè. Successivamente, l'algoritmo unisce tutti i cluster più vicini. Viene anche chiamato bottom-up.
\item \textbf{Divisivo}: tale approccio opera in maniera opposta rispetto al precedente, poichè tutti gli elementi vengono considerati come un unico cluster e l'algoritmo deve dividere il cluster in insiemi aventi dimensioni inferiori. Questa metodologia viene anche chiamata top-down.
\end{itemize}
Durante l'aggregazione degli elementi è necessario usare una funzione che permette di calcolare la similarità (o meglio la distanza) tra due cluster: questo permette all'algoritmo di unire i cluster simili. 

\subsection{Funzioni (o misure) di distanza}
\label{distancefunctions}
A seconda dell'approccio utilizzato, vi sono delle funzioni (o misure) che permettono di calcolare la distanza tra due cluster. Viene molto usato dagli algoritmi di clustering gerarchico per calcolare la similarià tra i cluster e per unire, eventualmente, i cluster simili.\\
Le funzioni di distanza usate da questo tipo di clustering sono: \textbf{single-link proximity}, \textbf{average-link proximity}, \textbf{complete-link proximity} e la \textbf{distanza tra centroidi}.

\paragraph{Single-link proximity}
Questa funzione calcola la distanza tra due cluster come la distanza minima tra elementi appartenenti a cluster differenti.
\begin{equation}
    D(C_i, C_j) = min_{x \in C_i, y \in C_j} d(x, y)
\end{equation}
\begin{figure}[h]
	\centering
	\includegraphics[width = 60mm]{clustering_single}
	\caption{Prossimità di tipo Simgle-link}
	\label{single}
\end{figure}

\paragraph{Average-link proximity}
Questa funzione calcola la distanza tra due cluster come la media delle distanze tra i singoli elementi.
\begin{equation}
    D(C_i, C_j) = \frac{1}{(|C_i||C_j|)} \sum_{x \in C_i, y \in C_j} d(x, y)
\end{equation}
\begin{figure}[h]
	\centering
	\includegraphics[width = 60mm]{clustering_average}
	\caption{Prossimità di tipo Average-link}
	\label{average}
\end{figure}

\paragraph{Complete-link proximity}
Questa funzione calcola la distanza tra i due cluster, considerando la distanza massima tra gli elementi appartenenti ai due cluster.
\begin{equation}
    D(C_i, C_j) = max_{x \in C_i, y \in C_j} d(x, y)
\end{equation}
\begin{figure}[h]
	\centering
	\includegraphics[width = 60mm]{clustering_complete}
	\caption{Prossimità di tipo Complete-link}
	\label{complete}
\end{figure}

\paragraph{Distanza tra centroidi}
Questa, invece, è la distanza tra i due cluster prendendo in considerazione i centroidi degli stessi.
\begin{equation}
    D(C_i, C_j) = d\left ( \hat{c_i}, \hat{c_j} \right )
\end{equation}
\begin{figure}[h]
	\centering
	\includegraphics[width = 60mm]{clustering_centroid}
	\caption{Distanza tra centroidi}
	\label{centroid}
\end{figure}


Nei casi precedenti, $d(x, y)$ indica una qualsiasi funzione distanza, su uno spazio metrico, che può essere:
\begin{itemize}
	\item \textbf{Distanza euclidea}: chiamata anche norma 2, è la distanza calcolata tra due punti, che può essere misurata su uno spazio multidimensionale. Siano $P = (p_1, p_2, ..., p_n)$ e $Q = (q_1, q_2, ..., q_n)$ due punti, la distanza sarà:
    \begin{equation}
        \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2} = \sqrt{\sum_{k=1}^{k} (p_k - q_k)^2}
    \end{equation}

	\item \textbf{Distanza di Manhattan}: chiamata anche geometria del taxi o norma 1, è la distanza tra due punti calcolata come la somma del valore assoluto delle differenze delle loro coordinate. Siano $P_1 = (x_1, y_1)$, $P_2 = (x_2, y_2)$ due punti, la distanza sarà:
    \begin{equation}
        L_1(P_1, P_2) = |x_1 - x_2| + |y_1 - y_2|
    \end{equation}

	\item \textbf{Coseno di similarità}: tecnica euristica usata per misurare la distanza tra due vettori, che viene effettuata calcolando il coseno dell'angolo ivi compreso, che hanno l'origine coincidente con quello del sistema di assi e passano per i rispettivi elementi. Il valore risultante più sarà vicino ad 1, più i due elementi saranno simili tra loro. Siano A e B due vettori di attributi numerici, allora il coseno di similarità sarà calcolato mediante la formula:
    \begin{equation}
        \cos(\theta) = \frac{AB}{||A|| ||B||}
    \end{equation}

	\item \textbf{Distanza di Hamming}: misura il numero di sostituzioni necessarie per convertire una stringa nell'altra, oppure può essere vista come un reporting del numero degli errori che hanno trasformato una stringa nell'altra. La distanza di Hamming tra 10{\color{red}1}1{\color{red}1}01 e 10{\color{red}0}1{\color{red}0}01 è 2; oppure tra 2{\color{red}14}3{\color{red}8}96 e 2{\color{red}23}3{\color{red}7}96 è 3.
\end{itemize}

\subsection{Algoritmi usati}
\label{clusteringexperimentation}
In questa sezione vengono descritti gli algoritmi di Clustering usati sui dataset della sperimentazione presente in questa tesi. Il suo scopo è quello di analizzare la bontà dell'operazione di Clustering mediante apposite metriche. Gli elementi sui quali è stato effettuato il Clustering sono stati appresi da algoritmi di Machine Learning differenti.

\paragraph{K-Means}
K-Means è un algoritmo di clustering di tipo partizionale, in cui ogni cluster viene identificato mediante un centroide.
\\
Si basa sull'algoritmo di Lloyd e consiste in 3 step. Il primo step consiste nella scelta dei centroidi iniziali, che saranno K elementi, casuali o usando informazioni euristiche, scelti dal dataset. Successivamente, l'algoritmo assegna per ogni elemento il centroide più vicino e ne crea di nuovi dalla media di tutti i campioni, assegnati ai centroidi precedenti. Si ripete questa fase finché l'algoritmo non converge.\\
Il pregio principale di questo algoritmo è che converge molto velocemente: si è analizzato, infatti, che il numero di iterazioni che l'algoritmo esegue è minore del numero di elementi del dataset.\\
K-means, però, può essere molto lento nel caso peggiore e non garantisce il raggiungimento dell'ottimo globale: la bontà della soluzione dipende dal set di cluster iniziale. Inoltre, un altro svantaggio è che l'algoritmo richiede, in input, il numero dei cluster.

\begin{figure}[h]
	\centering
	\includegraphics[width = 140mm]{kmeans}
	\caption{Esempio di clustering utilizzando K-Means}
	\label{kmeans}
\end{figure}

\paragraph{HDBScan}
HDBScan è un algoritmo di clustering che estende DBScan, rendendolo di tipo gerarchico. Si parte in maniera simile a DBScan: lo spazio viene trasformato a seconda della densità e viene effettuato su di esso una prossimità a single-link.\\
Invece di richiedere come input il parametro $\epsilon$, che viene usato da DBScan per considerare gli elementi del vicinato appartenenti al cluster, viene creato un albero, il quale viene usato per selezionare i cluster più stabili e persistenti.
Al posto di $\epsilon$, quindi, viene richiesta la dimensione minima dei cluster per determinare quali gruppi non devono essere considerati come cluster, oppure per dividerli e formare nuovi cluster.\\
Questo algoritmo è molto efficace ed è il più veloce, sia di DBScan che di K-Means.

\begin{figure}[h]
	\centering
	\includegraphics[width = 140mm]{hdbscan}
	\caption{Albero generato da HDBScan per la creazione dei cluster}
	\label{hdbscan}
\end{figure}

%aggiungi visualizzazione dati

\bibliographystyle{plain}
\bibliography{./../Bibliografia}                % database di biblatex 

\end{document}