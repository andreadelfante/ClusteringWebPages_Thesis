Il Clustering delle pagine di un sito Web è un processo fondamentale nel Web Mining utile a valutare l'interazione tra le pagine, organizzare i contenuti del sito e capire come questo sia stato strutturato. 
\\
Una pagina Web è composta da varie rappresentazioni (Sezione \ref{webpagerapresentation}) che la caratterizzano e la diversificano dalle altre. Attualmente, queste proprietà vengono sfruttate dagli algoritmi di Clustering in maniera indipendente.
\\
Gli obiettivi di questa tesi, descritti nella Sezione \ref{aimthesis}, sono stati raggiunti mediante l'utilizzo di una metodologia composta da 3 passi:
\begin{itemize}
	\item \textit{Crawling del sito Web}
	\item \textit{Costruzione del Dataset}
	\item \textit{Clustering delle pagine Web}
\end{itemize}
Di seguito, si descrivono in dettaglio le fasi della metodologia.

\section{Web Crawling}
Le proprietà che caratterizzano le pagine Web rendono complicato il processo di estrazione di informazioni, soprattutto nel caso in cui i contenuti vengono generati dinamicamente. Per analizzare i contenuti della rete e delle pagine Web si utilizza un software automatizzato chiamato Web Crawler. Tipicamente, questo programma viene usato per molti altri scopi, come l'indicizzazione di pagine Web. I motori di ricerca sfruttano tali indici per aumentare l'efficienza delle interrogazioni che gli utenti effettuano durante la navigazione.
\\
La ragione principale dell'uso dei Crawler è che il World Wide Web non è un contenitore centralizzato: può essere visto, infatti, come un insieme di siti Web che forniscono differenti servizi.
\\\\
L'algoritmo di Crawling è relativamente semplice: dato un insieme di URL di pagine Web, vengono scaricate tutte le pagine associate all'indirizzo URL, estratti gli hyperlinks e, iterativamente, scaricate le pagine associate a questi link. Il loro contenuto verrà analizzato e salvato per essere successivamente indicizzato. Nonostante l'apparente semplicità di questo algoritmo, l'attività di Web Crawling è caratterizzata da obiettivi inerenti \cite{crawling}:
\begin{itemize}
	\item \textbf{Scalabilità}. Il Web è molto grande e in continua evoluzione. I Crawler che cercano una copertura ampia ed aggiornata devono raggiungere throughput (prestazioni) estremamente alti. Per riuscirci, bisogna risolvere problematiche ingegneristiche complesse. I moderni motori di ricerca impiegano migliaia di computer e decine di collegamenti di rete ad alta velocità.
	\item \textbf{Compromessi per la selezione dei contenuti}. Persino i Crawler con un alto throughput non sono in grado di scandire l'intero Web o tenere traccia di tutti i cambiamenti. Per questo, il processo di Crawling viene effettuato in maniera selettiva con un attento e controllato ordine. Gli obiettivi sono di acquisire velocemente i contenuti aventi un alto valore, di assicurare la copertura di tutti i contenuti scelti, di ignorare quelli a bassa qualità, irrilevanti, ridondanti e dannosi e di mantenerli aggiornati. Il Crawler deve rispettare alcuni vincoli come il numero massimo di visite per sito e le pagine da scartare durante l'analisi.
	\item \textbf{Obblighi sociali}. I Crawler dovrebbero essere dei "buoni cittadini" del Web: non devono sovraccaricare i siti che scandiscono. Infatti, senza i giusti meccanismi, un throughput alto può inavvertitamente provocare un attacco Denial of Service (DOS), sovraccaricando il server Web e rendendolo inaccessibile.
	\item \textbf{Avversari}. Alcuni siti Web cercano di iniettare contenuti inutili o fuorvianti nel corpus assemblato dai Crawler. Questo comportamento è spesso motivato da incentivi finanziari, per esempio per indirizzare male il traffico verso siti commerciali.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width = 100mm]{WebCrawlerArchitecture}
	\caption{Architettura di un web Crawler}
	\label{crawlerarchitecture}
\end{figure}

\subsection{Crawling delle pagine di un sito Web}
Ai fini sperimentali è stato utilizzato un Web Crawler che estrae gli hyperlink di un sito Web per effettuare successivamente l'operazione di Clustering delle pagine associate a tale sito. Un sito Web può essere formalmente descritto come un grafo orientato $G = (V, E)$, dove $V$ è l'insieme delle pagine appartenenti al sito ed $E$ è l'insieme degli hyperlink. In molti casi, la homepage $h$ di un sito rappresenta il punto di inizio, tecnicamente espresso come un grafo orientato radicato (albero), che consente l'esplorazione da parte degli utenti.
\\
Il grafo del sito Web può essere ricco di link rumore (e.g. hyperlink scorciatoia) che non sono rilevanti nel processo di Clustering \cite{crescenzi} e vengono esclusi dal Crawling. In più, la struttura del sito Web è codificata in sistemi di navigazione che offrono una visuale della sua organizzazione. Questi sistemi vengono implementati come collezioni di link che hanno lo stesso dominio e condividono il layout e le proprietà visuali.

\begin{algorithm}[h]
	\caption{crawlingWebsite(homepage)}
	
	\begin{algorithmic}
		\State \textbf{Input:} URL homepage
		\State \textbf{Output:} Set$<$(URL, URL)$>$ E; Set$<$(URL, String)$>$ V
		\State $frontier \gets Set()$
		\State $Q \gets Queue(homepage)$
		
		\Repeat
			\State $currentPage \gets Q.dequeue()$
			\State $text \gets currentPage.getText()$
			\State $V.add((currentPage, text))$
			\State $webLists \gets extractWebLists(currentPage)$
			
			\For{\textbf{each} $a \in b$}
				\State $pagesToAnalyze \gets list.filterNot(page \to frontier.contains(page))$
				\State $Q.enqueue(pagesToAnalyze$
				\State $frontier.add(pagesToAnalize)$
				
				\For{\textbf{each} $u \in pagesToAnalyze$}
					\State $E.add((currentPage, u))$
				\EndFor
			\EndFor
		\Until{$!Q.empty()$}
		\State \textbf{return} $(V, E)$
	\end{algorithmic}
\end{algorithm}

La soluzione utilizzata per indicizzare un sito Web è stata quella di sfruttare il concetto di \textbf{lista Web}.
\\
Per definizione, una lista Web è una collezione di due o più elementi web che hanno struttura HTML simile, visualmente adiacenti ed allineati sulla pagina renderizzata. Questo allineamento può essere visto sia lungo l'asse x (i.e. una lista verticale), sia lungo l'asse y (i.e. una lista orizzontale), o ancora in maniera mista (i.e. griglia).

\begin{figure}[h]
	\centering
	\includegraphics[width = 75mm]{weblistexample}
	\caption{Esempio di liste Web}
	\label{weblistfigure}
\end{figure}

In Figura \ref{weblistfigure} vengono mostrate le liste Web, usate per guidare il processo di Crawling. I link inseriti nel box $A$ sono stati esclusi perché il loro dominio è differente da quello della homepage. Il risultato del Crawler è un sottografo $G' = (V', E')$, dove $V' \subseteq V$ ed $E' \subseteq E$.

\subsection{Normalizzazione degli URL}
Una volta effettuato il processo di Crawling ed estratti gli URL, si procede con la normalizzazione degli stessi. Questo è un processo in cui gli URL vengono modificati e standardizzati in maniera consistente. Il suo obiettivo è poter determinare se due URL, che sono sintatticamente differenti, possono essere equivalenti.
\\
I Crawler effettuano un qualche tipo di normalizzazione degli URL in modo da evitare che il processo di Crawling non vada ad analizzarli più volte.
\\\\
\texttt{http://www.facebook.com/}
\\
\texttt{facebook.com/}
\\\\
\texttt{http://208.77.188.166/}
\\
\texttt{http://www.example.com/}
\\\\
La normalizzazione di URL, come si evince dagli esempi sopra riportati, può comprendere sia la rimozione del protocollo ("http://") e della stringa "www", oppure la sostituzione dell'indirizzo IP con il nome del dominio. E' bene sottolineare come le due coppie in analisi puntino a due siti Web, rispettivamente \textit{Facebook} ed \textit{example}.
\\
Ci sono diverse modalità di normalizzazione che possono essere effettuate, fra cui la conversione degli URL in minuscolo e la rimozione del "." e ".." per portare gli URL da assoluti a relativi, aggiungere slash finali al componente di percorso non vuoto.
\\
Per la sperimentazione si è scelto di normalizzare gli URL eliminando la dicitura del protocollo ("http://" o "https://"), del "www" e dello slash finale.

\section{Costruzione del dataset}
Il Crawler, a seguito dell'analisi del sito, produce un grafo delle pagine Web ed il contenuto testuale di ogni pagina esplorata. Il grafo del sito Web servirà per generare le sequenze attraverso i Random Walks.
\\
Di seguito verrà spiegato il concetto di Random Walk e come questi sono stati utilizzati ai fini della sperimentazione.

\subsection{Random Walks}
\label{randomwalkssection}
Un Random Walk, o passeggiata aleatoria, è la formalizzazione dell'idea di effettuare passi successivi in direzioni casuali. Dal punto di vista matematico è il processo stocastico più semplice, ovvero quello markoviano, nel quale la probabilità che determina il passaggio ad uno stato, dipende solo dallo stato immediatamente precedente, e non dal come si è giunti a tale stato.
\\\\
In una passeggiata aleatoria monodimensionale si studia il moto di una particella puntiforme vincolata a muoversi lungo una retta nelle due direzioni consentite. Ad ogni movimento, questa si sposta a caso o a destra, con una probabilità fissata \textbf{p}, oppure a sinistra, con una probabilità \textbf{1 - p}, ed ogni passo è di lunghezza uguale e indipendente dagli altri.

\begin{figure}[ht!]
	\centering
	\includegraphics[width = 70mm]{randomwalkmono}
	\caption{Esempio di otto Random Walks in una dimensione}
	\label{randomwalkmono}
\end{figure}

In una passeggiata aleatoria bidimensionale si studia il moto di una particella vincolata a muoversi sul piano spostandosi casualmente ad ogni passo a destra, a sinistra, in alto o in basso con probabilità \textbf{1/2}. In particolare, ad ogni passo, la particella può compiere un movimento lungo una delle quattro diagonali con probabilità \textbf{1/4}. Ma qual è la probabilità che la particella torni al punto di partenza? In questo caso, la particella, che è libera di camminare casualmente con uguale probabilità nelle quattro direzioni, tornerà infinite volte al punto di partenza.

\begin{figure}[ht!]
	\centering
	\includegraphics[width = 70mm]{randomwalkbi}
	\caption{Esempio di Random Walks in due dimensioni}
	\label{randomwalkbi}
\end{figure}

In una passeggiata aleatoria tridimensionale si studia il moto di una particella vincolata a muoversi nello spazio spostandosi casualmente ad ogni passo a destra, a sinistra, in alto, in basso, in su o in giù con probabilità \textbf{1/2}. In pratica, ad ogni passo può compiere un movimento lungo una delle otto diagonali con probabilità \textbf{1/8}. Come nel caso precedente, è stata calcolata la probabilità che la particella torni prima o poi al punto di partenza ed è pari a \textbf{0,239}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width = 70mm]{randomwalktri}
	\caption{Esempio di Random Walks in tre dimensioni}
	\label{randomwalktri}
\end{figure}

Il concetto di Random Walk è stato applicato nei campi più disparati, alcuni dei quali sono:
\begin{itemize}
	\item \textbf{Economia finanziaria}. I Random Walk sono stati usati per modellare i prezzi delle azioni sui mercati azionari, tassi di cambio di moneta e materie prime.
	\item \textbf{Genetica}. Descrivono le proprietà statistiche della deriva generica, ovvero una componente dell'evoluzione di una specie dovuta a fattori casuali, che può essere studiata con metodi statistici.
	\item \textbf{Fisica}. Usati come modelli semplificati per studiare il moto browniano, ovvero il moto della particelle presenti in fluidi che è osservabile al microscopio.
	\item \textbf{Ecologia matematica}. Usati per descrivere i movimenti dei singoli animali, i processi di diffusione della materia, o ancora per la dinamica della popolazione. Quest'ultimo studia i cambiamenti del numero di individui, della densità e della struttura di una o diverse popolazioni. Inoltre analizza i processi biologici e ambientali che influenzano queste trasformazioni.
	\item \textbf{Informatica}. Usati per stimare la dimensione del Web, ovvero il numero di pagine e di siti che fanno parte del World Wide Web.
\end{itemize}

\subsection{Generazione delle sequenze}
Una passeggiata aleatoria sulla struttura ad hyperlink di un sito Web si basa sull'idea che le connessioni tra nodi (i.e. le pagine del sito) presentano delle informazioni latenti circa la loro correlazione. Per catturarle si è deciso di utilizzare un Random Walker, una componente incaricata di effettuare le passeggiate aleatorie sul grafo di un sito Web. Questa scelta è motivata dal fatto che le metodologie selettive per esplorare un sito Web, che derivano dalla teoria dei grafi, prevedono l'esplorazione di tutte le possibili opzioni per arrivare alla soluzione. Ma tali tecniche sono difficilmente computabili in quanto ricadono nella classe di complessità NP-completa.
\\
Il Random Walker, anche se permette di avere buone approssimazioni nell'esplorazione del grafo del sito, presenta una problematica: potrebbe capitare che la pagina che sta analizzando non presenta link al suo interno. La soluzione più diffusa è quella di effettuare un "salto" verso una qualsiasi altra pagina. Nel nostro caso, invece, dato che le sequenze da generare devono avere una lunghezza massima fissata prima della generazione, se l'attraversatore casuale incontra una pagina priva di hyperlink, allora si blocca semplicemente. La sequenza finale quindi, risulterà più piccola. Questa scelta è stata presa poiché l'informazione cercata nasce da percorsi reali di navigazione. Inoltre non vi è la necessità di una lunghezza obbligatoria da rispettare, in quanto le sequenze possono essere viste come frasi di un testo, dove le parole sono gli URL.

\begin{algorithm}[h]
	\caption{rwrGeneration(rwrLength, dbLength, G, $\alpha$)}
	\label{rwrGeneration}
	
	\begin{algorithmic}
		\State \textbf{Input:} int rwrLength \Comment{numero di passi massimo}
		\State \textbf{Input:} int dbLength \Comment{numero di frasi}
		\State \textbf{Input:} Graph G \Comment{il grafo del sito Web}
		\State \textbf{Input:} float $\alpha$
		\State \textbf{Output:} List$<$List$<$URL$>$$>$ randomWalks
		
		\For{\textbf{each} $i \in Range(0, dbLength)$}
			\State $w \gets List()$
			\State $w[0] \gets G.getRandomVertex()$

			\For{\textbf{each} $j \in Range(1, rwrLength)$}
				\State $\lambda \gets Math.random()$
				\If{$\lambda > \alpha$}
					\State $w[j] \gets G.getRandomOutlink(w[j-1])$
				\Else
					\State $w[j] \gets w[0]$
				\EndIf
			\EndFor
			
			\State $randomWalks.add(w)$
		\EndFor
		
		\State \textbf{return} $randomWalks$
	\end{algorithmic}
\end{algorithm}

Per motivi di sperimentazione sono stati implementati tre tipi diversi di Random Walk, utilizzabili modificando i parametri di esecuzione dell'Algoritmo \ref{rwrGeneration}.

\begin{itemize}
	\item \textbf{Random Walk standard}. Il caso standard prevede che si parta da un nodo casuale del grafo e si segua ogni volta un arco a caso fra quelli disponibili, fino al raggiungimento della lunghezza prefissata o all'impossibilità di proseguire.
	
	\item \textbf{Random Walk con partenza da homepage}. Con quest'altra modalità si ha una partenza fissata. Si parte, quindi, da un nodo prefissato del grafo, generalmente la homepage del sito Web in analisi, in modo da esplorare più percorsi possibili.
	
	\item \textbf{Random Walk attraverso le Liste}. Questo è il caso in cui si può eseguire uno delle due tipologie di Random Walks viste in precedenza, ma aventi il vincolo delle liste: l'algoritmo, quindi, opererà solo su un sottoinsieme di quello prodotto dalla metodologia scelta.
\end{itemize}

Quindi, i Random Walks prodotti sono stati trattati come frasi, in cui le parole sono i codici univoci degli URL. L'utilizzo di tali codici ha permesso di ridurre lo spazio in memoria e i tempi di elaborazione.
\\
Le frasi generate dal Random Walk standard sono state sfruttate da Word2Vec per apprendere la struttura del grafo del sito Web. Inoltre, per raggiungere uno degli obiettivi di questa tesi, è stata modificata l'implementazione di Word2Vec e successivamente applicata sui Random Walk con partenza fissa (dalla homepage). La stessa cosa è stata effettuata utilizzando le Liste Web.
\\
Nella sezione successiva viene spiegata, in maniera approfondita, la modifica di Word2Vec.

\subsection{Modifica dell'implementazione di Word2Vec}
\label{modifiedSkipgram}
Un aspetto particolare preso in considerazione in questa tesi è stato quello di chiedersi se, modificando in maniera opportuna l'algoritmo di Word Embedding Word2Vec, si potesse avere un miglioramento nel processo di Clustering delle pagine del grafo del sito Web in analisi.
\\
Per rispondere a questa domanda, si è deciso di analizzare e modificare il modello di apprendimento di Word2Vec Skip-Gram della libreria \texttt{deeplearning4j}. Questo modello consiste nel predire il contesto a partire da una parola. Per una argomentazione più approfondita si veda la Sezione \ref{word2vec}.
\\
La modifica di Skip-Gram consiste nel limitare l'apprendimento dell'algoritmo in maniera tale che venga analizzato solo il contesto sinistro, data una parola. E' importante sottolineare, inoltre, come questo modello sia stato ottimizzato con un valore chiamato \textbf{b}, che permette di aumentare o diminuire la finestra di contesto, dando più importanza alle parole più vicine a quella in analisi.
\\
Per la sperimentazione sono stati prodotti, quindi, due tipologie di modelli di Skip-Gram che analizzano solo il contesto sinistro: uno che utilizza il valore b ed un'altro che non lo usa.

\subsection{Scaling degli embeddings}
\label{scaling}
Una volta prodotti gli spazi vettoriali delle parole, sono state analizzate metodologie di Feature Scaling per capire la migliore da usare per la sperimentazione. 
\\
Il Feature Scaling è un metodo usato per standardizzare un intervallo di variabili indipendenti o features di dati. Viene anche chiamato normalizzazione di dati ed è generalmente effettuato nei passi di preprocessing di dati. Non solo, normalizzare i dati significa anche ridurre l'effetto degli Outlier. Il termine Outlier è usato in statistica per definire, in un insieme di osservazioni, un valore anomalo e aberrante (i.e. un valore chiaramente distante dalle altre osservazioni disponibili). Un numero consistente di Outlier nel campione in analisi può portare a risultati fuorvianti. Per esempio, se misurassimo la temperatura di 10 oggetti presenti in una stanza, la maggior parte dei quali risultasse avere una temperatura compresa fra i 20 e 25 gradi Celsius, allora il forno acceso, avente una temperatura di 350 gradi, sarebbe un dato aberrante.
\\\\
Le tecniche di Scaling più utilizzate sono:
\begin{itemize}
	\item \textbf{Z-score}. Con questa metodologia, i vettori vengono standardizzati ed avranno le proprietà di una distribuzione normale standardizzata, particolarmente utile nelle operazioni di stima statistica. Sono curve simmetriche con valori più concentrati verso il centro e meno nelle estremità laterali. Un esempio è la curva di Gauss.
	\\
	Si avranno vettori aventi $\mu = 0$ e $\sigma = 1$, dove $\mu$ è la media e $\sigma$ è la deviazione standard dalla media. Gli Z-score sono calcolati come segue:
	\begin{equation}
		z = \frac{x - \mu}{\sigma}
	\end{equation}
	\item \textbf{Min-Max}. E' un approccio alternativo allo Z-score visto in precedenza. I dati vengono normalizzati usando un intervallo fissato, generalmente tra 0 ed 1, dove 0 è il valore minimo ed 1 quello massimo. Viene applicata la formula:
	\begin{equation}
		X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
	\end{equation}
	\item \textbf{L2}. Viene chiamato anche Normalizzazione Euclidea. Questo metodo è una normalizzazione vettoriale. Dato un vettore $x = [x_1, x_2, ..., x_n]$, è possibile calcolare il valore della norma L2 con la seguente formula:
	\begin{equation}
		|x| = \sqrt{\sum_{k=1}^{n} x_k^2}
	\end{equation}
	Infine, per normalizzarlo, bisogna dividere ogni valore di x con quello ottenuto dalla formula di norma L2.
	\\
	In uno spazio vettoriale occorre calcolare il valore di norma L2 per ogni riga o colonna della matrice, a seconda della scelta e dividere ogni elemento della riga o della colonna per quel valore.
\end{itemize}

Ai fini della sperimentazione, alla luce di quanto dichiarato dagli autori di \cite{norm}, si è deciso di utilizzare come tecnica la L2 per ogni riga. Infatti gli stessi autori hanno affermato che, dopo una serie di tentativi, la norma L2 fornisce risultati di normalizzazione superiori se applicata per ogni riga della matrice.

\section{Web page Clustering}
L'operazione di Clustering delle pagine di un sito Web, generalmente, può avvenire sfruttando la struttura connessa del sito o trattando le singole pagine come documenti. Nel primo caso, vengono applicate tecniche e metodologie derivanti dalla teoria dei grafi per partizionare il grafo e raggruppare le pagine simili. Nel secondo caso, invece, viene analizzato il contenuto testuale visivo della pagina, ovvero tutto quello che l'utente può percepire durante la navigazione sulle pagine di un sito Web.
\\\\
Gli hyperlink tra le pagine vengono usati per organizzare i contenuti, puntando a pagine differenti. Il contenuto testuale, data l'ambiguità del linguaggio naturale, può fornire indizi sbagliati e considerare diverse pagine correlate solo per una differente distribuzione dei termini.
\\\\
In questa tesi si è scelto di utilizzare algoritmi di Clustering per raggruppare le pagine di un sito Web in base alla loro rappresentazione vettoriale. I vettori utilizzati per l'operazione di Clustering sono:
\begin{itemize}
	\item \textbf{Vettori dei link}. Sono state utilizzate tre tipologie di Word2Vec: Skip-Gram che considera solo il contesto sinistro ottimizzato (utilizzando il valore della b) che apprende da sequenze generate da Random Walk partendo dalla homepage; Skip-Gram che considera solo il contesto sinistro non ottimizzato (ignorando la b) che apprende da sequenze generate da Random Walk partendo dalla homepage; Skip-Gram che considera il contesto sia destro sia sinistro che apprende da sequenze generate da Random Walk standard.
	\\
	E' stato inoltre applicato sul grafo del sito Web LINE (Sezione \ref{line}), un altro algoritmo che produce gli embedding dei nodi in base alla loro prossimità con gli altri. Questo algoritmo è stato applicato nelle sue due varianti: prossimità di primo e secondo ordine.
	%\\
	%La differenza tra Word2Vec e LINE sta nel fatto che il primo considera relazioni indirette poiché l'apprendimento dipende dalla lunghezza dei passi generati dal Random Walker e dalla dimensione della finestra di contesto; il secondo, invece, analizza relazioni dirette o al più relazioni tra nodi fratelli: rispettivamente la prossimità di primo e di secondo ordine.
	\\
	I vettori dei link sono stati normalizzati utilizzando L2 per ogni riga.
	\item \textbf{Vettori dei contenuti}. Le pagine del sito Web sono state opportunamente preprocessate per essere trattate come documenti. Sono stati rimossi i tag HTML, i caratteri di escape e non alfanumerici e le parole troppo frequenti ($> 90\%$) e poco frequenti ($< 5\%$).
	\\
	Successivamente, sono stati applicati Doc2Vec (in Sezione \ref{doc2vec}) e TF-IDF (in Sezione \ref{tfidf}) sulla pagina Web preprocessata. I vettori dei contenuti sono stati normalizzati utilizzando L2 per ogni riga.
	\item \textbf{Vettori dei link-contenuti}. Questa tipologia di vettori è stata prodotta andando a concatenare i vettori dei link e quelli dei contenuti aventi stesso codice identificativo dell'URL. Il risultato sarà un nuovo spazio vettoriale i cui vettori avranno dimensione $n + m$, dove $n$ è la lunghezza del vettore dei link ed $m$ è la lunghezza del vettore dei contenuti. Prima della concatenazione, i vettori di link e di contenuto sono stati normalizzati utilizzando L2 per ogni riga.
\end{itemize}

\section{Esempio di dataset}
Questa sezione ha il compito di spiegare come è stato strutturato il dataset per la sperimentazione. 
I file sono stati generati dal Web Crawling e dai Random Walks, usati per effettuare la sperimentazione descritta in questa tesi.
\\
Per spiegare il dataset è stato utilizzato come esempio il sito del dipartimento di informatica di Stanford, CA: \texttt{cs.stanford.edu}.

\paragraph{seedsMap.txt}
Questo file contiene le associazioni tra gli URL e il codice identificativo, usato per ridurre i tempi di elaborazione e spazio di archiviazione.
\\\\
\texttt{
cs.stanford.edu/csdcf/policies	73\\
cs.stanford.edu/admissions/reapplying	52\\
cs.stanford.edu/people/eaf/wordpress/videos	247\\
...
}

\paragraph{vertex.txt}
Contiene il contenuto testuale di ogni pagina esplorata. Ogni riga è formata dal codice identificativo di un URL ed il relativo contenuto.
\\\\
\texttt{
3	skip to skip to content skip to navigation webauth login ...\\
121	webauth error webauth error an error has occurred error ...\\
90	skip to content skip to navigation webauth login sunetid ...\\
...
}

\paragraph{edges.txt}
Contiene gli archi tra i nodi del grafo del sito web, i quali non sono altro che i collegamenti tra le pagine. Questo file viene usato per la generazione delle sequenze dall'algoritmo dei Random Walks.
\\\\
\texttt{
3	69\\
3	101\\
3	88\\
...
}

\paragraph{sequenceIDs.txt}
Contiene le sequenze generate da un Random Walker. Nel file vengono riportati i passi generati dall'algoritmo di Random Walk partendo da un nodo casuale del grafo del sito Web.
\\\\
\texttt{
109 33 106 89 10 108 57 91 8 51\\
80 42 17 95 66 109 109 78 44 22\\
206 280\\
...
}

\paragraph{sequenceIDsFromHomepage.txt}
Contiene le sequenze generate da un Random Walker. Nel file vengono riportati i passi generati dall'algoritmo di Random Walk specificando il nodo di partenza (i.e. la homepage) di ogni sequenza.
\\\\
\texttt{
3 37 72\\
3 12 48 47 22 64 74 48 36 8\\
3 6 26 12 57 63 95 109 87 71\\
...
}

\paragraph{embeddings\_with\_b.txt, embeddings\_no\_b.txt, embeddings\_normal.txt, embeddings\_line\_first.txt,\\ embeddings\_line\_second.txt, embeddings\_doc2vec.txt}
\texttt{\\embeddings\_with\_b.txt} ed \texttt{embeddings\_no\_b.txt} contengono gli embedding degli URL appresi da sequenze generate da Random Walk con partenza da homepage, che sono stati generati dal modello di apprendimento Skip-Gram, rispettivamente ottimizzato e non, che considera solo il contesto sinistro, data una parola.
\\
\texttt{embeddings\_normal.txt} contiene gli embedding degli URL appresi da sequenze prodotte da Random Walk standard, che sono stati generati dal modello di apprendimento Skip-Gram che considera il contesto destro e sinistro, data una parola.
\\
\texttt{embeddings\_line\_first.txt} ed \texttt{embeddings\_line\_second.txt} contengono gli embedding degli URL appresi dal file degli archi del grafo del sito Web. E' stato utilizzato LINE rispettivamente con prossimità di primo e secondo ordine.
\\
\texttt{embeddings\_doc2vec.txt} contiene gli embedding del contenuto delle pagine del grafo del sito Web, dove ognuno dei vettori è associato ad un codice identificativo dell'URL.
\\\\
\texttt{
80 0.030692825093865395 0.09835819154977798 ...\\
44 -0.06424273550510406 -0.0433584563434124 ...\\
69 -0.1409958302974701 -0.013164487667381763 ...\\
...
}

\paragraph{groundTruth.csv}
Contiene la tavola di verità, in cui ad ogni URL è associata una etichetta che indica il cluster di appartenenza. Questo file viene utilizzato per misurare la bontà dell'algoritmo di Clustering ed è stato usato nella sperimentazione. Nella tavola di verità è presente come etichetta -1: se un URL presenta questo valore, significa che la pagina non è stata assegnata ad alcun raggruppamento.
\\\\
\texttt{
cs.stanford.edu/ip	-1\\
cs.stanford.edu/about/contact-us	1\\
cs.stanford.edu/academics	2\\
cs.stanford.edu/academics/phd	2\\
cs.stanford.edu/admissions	3\\
cs.stanford.edu/computing-guide	4\\
...
}